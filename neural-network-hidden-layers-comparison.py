# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17NRVg6oT9CqhUoPX25lxpJUaSqRz4gUz
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, ELU
from tensorflow.keras.optimizers import Adam

# 1. Generate nonlinear dataset
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)

# 2. Split data (30% for testing)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Function to build model with a given activation
def build_model(activation_name):
    model = Sequential()
    if activation_name == "leakyrelu":
        model.add(Dense(32, input_shape=(2,)))
        model.add(LeakyReLU(alpha=0.1))
        for _ in range(4):
            model.add(Dense(32))
            model.add(LeakyReLU(alpha=0.1))
    elif activation_name == "elu":
        model.add(Dense(32, input_shape=(2,)))
        model.add(ELU(alpha=1.0))
        for _ in range(4):
            model.add(Dense(32))
            model.add(ELU(alpha=1.0))
    else:
        model.add(Dense(32, activation=activation_name, input_shape=(2,)))
        for _ in range(4):
            model.add(Dense(32, activation=activation_name))

    model.add(Dense(1, activation='sigmoid'))  # Output layer
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# 5. Train models with different activations
activations = ["relu", "tanh", "sigmoid", "leakyrelu", "elu"]
histories = {}
models = {}

for act in activations:
    print(f"\nTraining model with activation: {act}")
    model = build_model(act)
    history = model.fit(
        X_train_scaled, y_train,
        epochs=20,
        batch_size=32,
        verbose=0,
        validation_split=0.2
    )
    models[act] = model
    histories[act] = history

    # ---- Plot Training vs Validation Loss for each activation ----
    epochs = range(1, 21)
    plt.figure(figsize=(6, 4))
    plt.plot(epochs, history.history['loss'], label='Train Loss')
    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
    plt.title(f'Loss vs Epochs ({act.upper()})')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# 6. Final Test Accuracy for each activation
for act in activations:
    loss, acc = models[act].evaluate(X_test_scaled, y_test, verbose=0)
    print(f"Test accuracy with {act}: {acc:.3f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, ELU
from tensorflow.keras.optimizers import Adam

# 1. Generate nonlinear dataset
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)

# 2. Split data (30% for testing)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Function to build model with one hidden layer and different activations
def build_model(activation_name):
    model = Sequential()

    # --- One hidden layer ---
    if activation_name == "leakyrelu":
        model.add(Dense(32, input_shape=(2,)))
        model.add(LeakyReLU(alpha=0.1))
    elif activation_name == "elu":
        model.add(Dense(32, input_shape=(2,)))
        model.add(ELU(alpha=1.0))
    else:
        model.add(Dense(32, activation=activation_name, input_shape=(2,)))

    # Output layer
    model.add(Dense(1, activation='sigmoid'))

    # Compile
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# 5. Train models with different activations
activations = ["relu", "tanh", "sigmoid", "leakyrelu", "elu"]
histories = {}
models = {}

for act in activations:
    print(f"\nTraining model with activation: {act}")
    model = build_model(act)
    history = model.fit(
        X_train_scaled, y_train,
        epochs=20,
        batch_size=32,
        verbose=0,
        validation_split=0.2
    )
    models[act] = model
    histories[act] = history

    # ---- Plot Training vs Validation Loss for each activation ----
    epochs = range(1, 21)
    plt.figure(figsize=(6, 4))
    plt.plot(epochs, history.history['loss'], label='Train Loss')
    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
    plt.title(f'Loss vs Epochs ({act.upper()})')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# 6. Final Test Accuracy for each activation
for act in activations:
    loss, acc = models[act].evaluate(X_test_scaled, y_test, verbose=0)
    print(f"Test accuracy with {act}: {acc:.3f}")

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

# 1. Define a simple Feedforward Network
class SimpleNN(nn.Module):
    def _init_(self, hidden_size=128, activation='relu'):
        super(SimpleNN, self)._init_()
        self.fc1 = nn.Linear(28*28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)
        self.activation = activation

    def forward(self, x):
        x = x.view(-1, 28*28)
        if self.activation == 'relu':
            x = F.relu(self.fc1(x))
        elif self.activation == 'sigmoid':
            x = torch.sigmoid(self.fc1(x))
        elif self.activation == 'tanh':
            x = torch.tanh(self.fc1(x))
        x = self.fc2(x)
        return x

# 2. Training Function
def train_model(hparams):
    # Unpack hyperparameters
    lr = hparams['lr']
    batch_size = hparams['batch_size']
    hidden_size = hparams['hidden_size']
    activation = hparams['activation']
    epochs = hparams['epochs']

    # Data loading
    transform = transforms.Compose([transforms.ToTensor()])
    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    # Model, optimizer, loss
    model = SimpleNN(hidden_size=hidden_size, activation=activation)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # Training
    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    accuracy = 100.0 * correct / total
    train_time = time.time() - start_time
    return accuracy, train_time

# 3. Experiment: Change Hyperparameters
base_params = {
    'lr': 0.001,
    'batch_size': 64,
    'hidden_size': 128,
    'activation': 'relu',
    'epochs': 3
}

results = {}

# (a) Learning Rate
learning_rates = [0.0005, 0.001, 0.005, 0.01]
lr_acc = []
for lr in learning_rates:
    params = base_params.copy()
    params['lr'] = lr
    acc, _ = train_model(params)
    lr_acc.append(acc)
results['Learning Rate'] = (learning_rates, lr_acc)

# (b) Batch Size
batch_sizes = [32, 64, 128, 256]
bs_acc = []
for bs in batch_sizes:
    params = base_params.copy()
    params['batch_size'] = bs
    acc, _ = train_model(params)
    bs_acc.append(acc)
results['Batch Size'] = (batch_sizes, bs_acc)

# (c) Hidden Neurons
hidden_sizes = [64, 128, 256, 512]
hs_acc = []
for hs in hidden_sizes:
    params = base_params.copy()
    params['hidden_size'] = hs
    acc, _ = train_model(params)
    hs_acc.append(acc)
results['Hidden Neurons'] = (hidden_sizes, hs_acc)

# (d) Activation Functions
activations = ['relu', 'sigmoid', 'tanh']
act_acc = []
for act in activations:
    params = base_params.copy()
    params['activation'] = act
    acc, _ = train_model(params)
    act_acc.append(acc)
results['Activation Function'] = (activations, act_acc)

# 4. Plot Results
plt.figure(figsize=(14, 10))
plt.subplot(2, 2, 1)
plt.plot(results['Learning Rate'][0], results['Learning Rate'][1], marker='o')
plt.title('Effect of Learning Rate on Accuracy')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 2)
plt.plot(results['Batch Size'][0], results['Batch Size'][1], marker='o')
plt.title('Effect of Batch Size on Accuracy')
plt.xlabel('Batch Size')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 3)
plt.plot(results['Hidden Neurons'][0], results['Hidden Neurons'][1], marker='o')
plt.title('Effect of Hidden Neurons on Accuracy')
plt.xlabel('Hidden Neurons')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 4)
plt.bar(results['Activation Function'][0], results['Activation Function'][1])
plt.title('Effect of Activation Function on Accuracy')
plt.xlabel('Activation Function')
plt.ylabel('Accuracy (%)')

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

# 1. Define a simple Feedforward Network
class SimpleNN(nn.Module):
    def__init__(self, hidden_size=128, activation='relu'):
        super(SimpleNN, self)._init_()
        self.fc1 = nn.Linear(28*28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)
        self.activation = activation

    def forward(self, x):
        x = x.view(-1, 28*28)
        if self.activation == 'relu':
            x = F.relu(self.fc1(x))
        elif self.activation == 'sigmoid':
            x = torch.sigmoid(self.fc1(x))
        elif self.activation == 'tanh':
            x = torch.tanh(self.fc1(x))
        x = self.fc2(x)
        return x

# 2. Training Function
def train_model(hparams):
    # Unpack hyperparameters
    lr = hparams['lr']
    batch_size = hparams['batch_size']
    hidden_size = hparams['hidden_size']
    activation = hparams['activation']
    epochs = hparams['epochs']

    # Data loading
    transform = transforms.Compose([transforms.ToTensor()])
    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    # Model, optimizer, loss
    model = SimpleNN(hidden_size=hidden_size, activation=activation)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # Training
    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    accuracy = 100.0 * correct / total
    train_time = time.time() - start_time
    return accuracy, train_time

# 3. Experiment: Change Hyperparameters
base_params = {
    'lr': 0.001,
    'batch_size': 64,
    'hidden_size': 128,
    'activation': 'relu',
    'epochs': 3
}

results = {}

# (a) Learning Rate
learning_rates = [0.0005, 0.001, 0.005, 0.01]
lr_acc = []
for lr in learning_rates:
    params = base_params.copy()
    params['lr'] = lr
    acc, _ = train_model(params)
    lr_acc.append(acc)
results['Learning Rate'] = (learning_rates, lr_acc)

# (b) Batch Size
batch_sizes = [32, 64, 128, 256]
bs_acc = []
for bs in batch_sizes:
    params = base_params.copy()
    params['batch_size'] = bs
    acc, _ = train_model(params)
    bs_acc.append(acc)
results['Batch Size'] = (batch_sizes, bs_acc)

# (c) Hidden Neurons
hidden_sizes = [64, 128, 256, 512]
hs_acc = []
for hs in hidden_sizes:
    params = base_params.copy()
    params['hidden_size'] = hs
    acc, _ = train_model(params)
    hs_acc.append(acc)
results['Hidden Neurons'] = (hidden_sizes, hs_acc)

# (d) Activation Functions
activations = ['relu', 'sigmoid', 'tanh']
act_acc = []
for act in activations:
    params = base_params.copy()
    params['activation'] = act
    acc, _ = train_model(params)
    act_acc.append(acc)
results['Activation Function'] = (activations, act_acc)

# 4. Plot Results
plt.figure(figsize=(14, 10))
plt.subplot(2, 2, 1)
plt.plot(results['Learning Rate'][0], results['Learning Rate'][1], marker='o')
plt.title('Effect of Learning Rate on Accuracy')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 2)
plt.plot(results['Batch Size'][0], results['Batch Size'][1], marker='o')
plt.title('Effect of Batch Size on Accuracy')
plt.xlabel('Batch Size')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 3)
plt.plot(results['Hidden Neurons'][0], results['Hidden Neurons'][1], marker='o')
plt.title('Effect of Hidden Neurons on Accuracy')
plt.xlabel('Hidden Neurons')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 4)
plt.bar(results['Activation Function'][0], results['Activation Function'][1])
plt.title('Effect of Activation Function on Accuracy')
plt.xlabel('Activation Function')
plt.ylabel('Accuracy (%)')

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

# 1. Define a simple CNN
class SimpleCNN(nn.Module):
    def __init__(self, hidden_size=128, activation='relu'):
        super(SimpleCNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # output 32x28x28
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # output 64x14x14 after pooling
        self.pool = nn.MaxPool2d(2, 2)

        # Fully connected layers
        self.fc1 = nn.Linear(64 * 7 * 7, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)

        # Store activation
        self.activation = activation

    def forward(self, x):
        # Conv + Pool
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))

        # Flatten
        x = x.view(-1, 64 * 7 * 7)

        # Activation choice for fc1
        if self.activation == 'relu':
            x = F.relu(self.fc1(x))
        elif self.activation == 'sigmoid':
            x = torch.sigmoid(self.fc1(x))
        elif self.activation == 'tanh':
            x = torch.tanh(self.fc1(x))

        # Output
        x = self.fc2(x)
        return x


# 2. Training Function
def train_model(hparams):
    lr = hparams['lr']
    batch_size = hparams['batch_size']
    hidden_size = hparams['hidden_size']
    activation = hparams['activation']
    epochs = hparams['epochs']

    # Data loading (with normalization for CNNs)
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    # Model, optimizer, loss
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SimpleCNN(hidden_size=hidden_size, activation=activation).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # Training
    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    accuracy = 100.0 * correct / total
    train_time = time.time() - start_time
    return accuracy, train_time


# 3. Hyperparameter experiments
base_params = {
    'lr': 0.001,
    'batch_size': 64,
    'hidden_size': 128,
    'activation': 'relu',
    'epochs': 3
}

results = {}

# (a) Learning Rate
learning_rates = [0.0005, 0.001, 0.005, 0.01]
lr_acc = []
for lr in learning_rates:
    params = base_params.copy()
    params['lr'] = lr
    acc, _ = train_model(params)
    lr_acc.append(acc)
results['Learning Rate'] = (learning_rates, lr_acc)

# (b) Batch Size
batch_sizes = [32, 64, 128, 256]
bs_acc = []
for bs in batch_sizes:
    params = base_params.copy()
    params['batch_size'] = bs
    acc, _ = train_model(params)
    bs_acc.append(acc)
results['Batch Size'] = (batch_sizes, bs_acc)

# (c) Hidden Neurons
hidden_sizes = [64, 128, 256, 512]
hs_acc = []
for hs in hidden_sizes:
    params = base_params.copy()
    params['hidden_size'] = hs
    acc, _ = train_model(params)
    hs_acc.append(acc)
results['Hidden Neurons'] = (hidden_sizes, hs_acc)

# (d) Activation Functions
activations = ['relu', 'sigmoid', 'tanh']
act_acc = []
for act in activations:
    params = base_params.copy()
    params['activation'] = act
    acc, _ = train_model(params)
    act_acc.append(acc)
results['Activation Function'] = (activations, act_acc)

# 4. Plot Results
plt.figure(figsize=(14, 10))
plt.subplot(2, 2, 1)
plt.plot(results['Learning Rate'][0], results['Learning Rate'][1], marker='o')
plt.title('Effect of Learning Rate on Accuracy (CNN)')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 2)
plt.plot(results['Batch Size'][0], results['Batch Size'][1], marker='o')
plt.title('Effect of Batch Size on Accuracy (CNN)')
plt.xlabel('Batch Size')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 3)
plt.plot(results['Hidden Neurons'][0], results['Hidden Neurons'][1], marker='o')
plt.title('Effect of Hidden Neurons on Accuracy (CNN)')
plt.xlabel('Hidden Neurons')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 4)
plt.bar(results['Activation Function'][0], results['Activation Function'][1])
plt.title('Effect of Activation Function on Accuracy (CNN)')
plt.xlabel('Activation Function')
plt.ylabel('Accuracy (%)')

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 1. Define a simple CNN
class SimpleCNN(nn.Module):
    def _init_(self, hidden_channels=16, activation='relu'):
        super(SimpleCNN, self)._init_()
        self.conv1 = nn.Conv2d(1, hidden_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, padding=1)
        self.fc1 = nn.Linear((hidden_channels*2) * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.activation = activation

    def forward(self, x):
        if self.activation == 'relu':
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
        elif self.activation == 'sigmoid':
            x = torch.sigmoid(self.conv1(x))
            x = torch.sigmoid(self.conv2(x))
        elif self.activation == 'tanh':
            x = torch.tanh(self.conv1(x))
            x = torch.tanh(self.conv2(x))
        x = F.max_pool2d(x, 2)  # downsample
        x = F.max_pool2d(x, 2)  # downsample again
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# 2. Training Function
def train_model(hparams):
    lr = hparams['lr']
    batch_size = hparams['batch_size']
    hidden_channels = hparams['hidden_channels']
    activation = hparams['activation']
    epochs = hparams['epochs']

    # Data loading
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    # âš¡ Subset for speed
    train_dataset = Subset(train_dataset, range(10000))
    test_dataset = Subset(test_dataset, range(2000))

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Model, optimizer, loss
    model = SimpleCNN(hidden_channels=hidden_channels, activation=activation).to(device)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # Training loop
    for epoch in range(epochs):
        model.train()
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    accuracy = 100. * correct / len(test_loader.dataset)
    return accuracy

# 3. Hyperparameter configs
hparams_list = [
    # Learning rate experiments
    {'lr': 0.001, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01,  'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.1,   'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},

    # Batch size experiments
    {'lr': 0.01, 'batch_size': 32,  'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 128, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 256, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},

    # Hidden channels experiments
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 8,  'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 32, 'activation': 'relu', 'epochs': 2},

    # Activation function experiments
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu',    'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'sigmoid', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'tanh',    'epochs': 2}
]

# 4. Run experiments
results = []
for hparams in hparams_list:
    acc = train_model(hparams)
    results.append((hparams, acc))
    print(f"Done: {hparams}, Accuracy={acc:.2f}%")

# 5. Collect results
lr_results = {}
bs_results = {}
hidden_results = {}
act_results = {}

for h, acc in results:
    if h['batch_size']==64 and h['hidden_channels']==16 and h['activation']=='relu':
        lr_results[h['lr']] = acc
    if h['lr']==0.01 and h['hidden_channels']==16 and h['activation']=='relu':
        bs_results[h['batch_size']] = acc
    if h['lr']==0.01 and h['batch_size']==64 and h['activation']=='relu':
        hidden_results[h['hidden_channels']] = acc
    if h['lr']==0.01 and h['batch_size']==64 and h['hidden_channels']==16:
        act_results[h['activation']] = acc

# 6. ðŸ“Š Plot graphs
plt.figure(figsize=(12,8))

plt.subplot(2,2,1)
plt.plot(list(lr_results.keys()), list(lr_results.values()), marker='o')
plt.xlabel("Learning Rate")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Learning Rate on Accuracy")
plt.grid(True)

plt.subplot(2,2,2)
plt.plot(list(bs_results.keys()), list(bs_results.values()), marker='o')
plt.xlabel("Batch Size")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Batch Size on Accuracy")
plt.grid(True)

plt.subplot(2,2,3)
plt.plot(list(hidden_results.keys()), list(hidden_results.values()), marker='o')
plt.xlabel("Hidden Channels")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Hidden Channels on Accuracy")
plt.grid(True)

plt.subplot(2,2,4)
plt.bar(list(act_results.keys()), list(act_results.values()))
plt.xlabel("Activation Function")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Activation Function on Accuracy")

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 1. Define a simple CNN
class SimpleCNN(nn.Module):
    def _init_(self, hidden_channels=16, activation='relu'):
        super(SimpleCNN, self)._init_()
        self.conv1 = nn.Conv2d(1, hidden_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, padding=1)
        self.fc1 = nn.Linear((hidden_channels*2) * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.activation = activation

    def forward(self, x):
        if self.activation == 'relu':
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
        elif self.activation == 'sigmoid':
            x = torch.sigmoid(self.conv1(x))
            x = torch.sigmoid(self.conv2(x))
        elif self.activation == 'tanh':
            x = torch.tanh(self.conv1(x))
            x = torch.tanh(self.conv2(x))
        x = F.max_pool2d(x, 2)  # downsample
        x = F.max_pool2d(x, 2)  # downsample again
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# 2. Training Function
def train_model(hparams):
    lr = hparams['lr']
    batch_size = hparams['batch_size']
    hidden_channels = hparams['hidden_channels']
    activation = hparams['activation']
    epochs = hparams['epochs']

    # Data loading
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    # âš¡ Subset for speed
    train_dataset = Subset(train_dataset, range(10000))
    test_dataset = Subset(test_dataset, range(2000))

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Model, optimizer, loss
    model = SimpleCNN(hidden_channels=hidden_channels, activation=activation).to(device)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # Training loop
    for epoch in range(epochs):
        model.train()
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    accuracy = 100. * correct / len(test_loader.dataset)
    return accuracy

# 3. Hyperparameter configs
hparams_list = [
    # Learning rate experiments
    {'lr': 0.001, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01,  'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.1,   'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},

    # Batch size experiments
    {'lr': 0.01, 'batch_size': 32,  'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 128, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 256, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},

    # Hidden channels experiments
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 8,  'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 32, 'activation': 'relu', 'epochs': 2},

    # Activation function experiments
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'relu',    'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'sigmoid', 'epochs': 2},
    {'lr': 0.01, 'batch_size': 64, 'hidden_channels': 16, 'activation': 'tanh',    'epochs': 2}
]

# 4. Run experiments
results = []
for hparams in hparams_list:
    acc = train_model(hparams)
    results.append((hparams, acc))
    print(f"Done: {hparams}, Accuracy={acc:.2f}%")

# 5. Collect results
lr_results = {}
bs_results = {}
hidden_results = {}
act_results = {}

for h, acc in results:
    if h['batch_size']==64 and h['hidden_channels']==16 and h['activation']=='relu':
        lr_results[h['lr']] = acc
    if h['lr']==0.01 and h['hidden_channels']==16 and h['activation']=='relu':
        bs_results[h['batch_size']] = acc
    if h['lr']==0.01 and h['batch_size']==64 and h['activation']=='relu':
        hidden_results[h['hidden_channels']] = acc
    if h['lr']==0.01 and h['batch_size']==64 and h['hidden_channels']==16:
        act_results[h['activation']] = acc

# 6. ðŸ“Š Plot graphs
plt.figure(figsize=(12,8))

plt.subplot(2,2,1)
plt.plot(list(lr_results.keys()), list(lr_results.values()), marker='o')
plt.xlabel("Learning Rate")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Learning Rate on Accuracy")
plt.grid(True)

plt.subplot(2,2,2)
plt.plot(list(bs_results.keys()), list(bs_results.values()), marker='o')
plt.xlabel("Batch Size")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Batch Size on Accuracy")
plt.grid(True)

plt.subplot(2,2,3)
plt.plot(list(hidden_results.keys()), list(hidden_results.values()), marker='o')
plt.xlabel("Hidden Channels")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Hidden Channels on Accuracy")
plt.grid(True)

plt.subplot(2,2,4)
plt.bar(list(act_results.keys()), list(act_results.values()))
plt.xlabel("Activation Function")
plt.ylabel("Accuracy (%)")
plt.title("Effect of Activation Function on Accuracy")

plt.tight_layout()
plt.show()





import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

# 1. Define a simple Feedforward Network
class SimpleNN(nn.Module):
    def __init__(self, hidden_size=128, activation='relu'):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28*28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)
        self.activation = activation

    def forward(self, x):
        x = x.view(-1, 28*28)
        if self.activation == 'relu':
            x = F.relu(self.fc1(x))
        elif self.activation == 'sigmoid':
            x = torch.sigmoid(self.fc1(x))
        elif self.activation == 'tanh':
            x = torch.tanh(self.fc1(x))
        x = self.fc2(x)
        return x

# 2. Training Function
def train_model(hparams):
    # Unpack hyperparameters
    lr = hparams['lr']
    batch_size = hparams['batch_size']
    hidden_size = hparams['hidden_size']
    activation = hparams['activation']
    epochs = hparams['epochs']

    # Data loading
    transform = transforms.Compose([transforms.ToTensor()])
    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    # Model, optimizer, loss
    model = SimpleNN(hidden_size=hidden_size, activation=activation)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # Training
    start_time = time.time()
    for epoch in range(epochs):
        model.train()
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

    # Evaluation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    accuracy = 100.0 * correct / total
    train_time = time.time() - start_time
    return accuracy, train_time

# 3. Experiment: Change Hyperparameters
base_params = {
    'lr': 0.001,
    'batch_size': 64,
    'hidden_size': 128,
    'activation': 'relu',
    'epochs': 3
}

results = {}

# (a) Learning Rate
learning_rates = [0.0005, 0.001, 0.005, 0.01]
lr_acc = []
for lr in learning_rates:
    params = base_params.copy()
    params['lr'] = lr
    acc, _ = train_model(params)
    lr_acc.append(acc)
results['Learning Rate'] = (learning_rates, lr_acc)

# (b) Batch Size
batch_sizes = [32, 64, 128, 256]
bs_acc = []
for bs in batch_sizes:
    params = base_params.copy()
    params['batch_size'] = bs
    acc, _ = train_model(params)
    bs_acc.append(acc)
results['Batch Size'] = (batch_sizes, bs_acc)

# (c) Hidden Neurons
hidden_sizes = [64, 128, 256, 512]
hs_acc = []
for hs in hidden_sizes:
    params = base_params.copy()
    params['hidden_size'] = hs
    acc, _ = train_model(params)
    hs_acc.append(acc)
results['Hidden Neurons'] = (hidden_sizes, hs_acc)

# (d) Activation Functions
activations = ['relu', 'sigmoid', 'tanh']
act_acc = []
for act in activations:
    params = base_params.copy()
    params['activation'] = act
    acc, _ = train_model(params)
    act_acc.append(acc)
results['Activation Function'] = (activations, act_acc)

# 4. Plot Results
plt.figure(figsize=(14, 10))
plt.subplot(2, 2, 1)
plt.plot(results['Learning Rate'][0], results['Learning Rate'][1], marker='o')
plt.title('Effect of Learning Rate on Accuracy')
plt.xlabel('Learning Rate')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 2)
plt.plot(results['Batch Size'][0], results['Batch Size'][1], marker='o')
plt.title('Effect of Batch Size on Accuracy')
plt.xlabel('Batch Size')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 3)
plt.plot(results['Hidden Neurons'][0], results['Hidden Neurons'][1], marker='o')
plt.title('Effect of Hidden Neurons on Accuracy')
plt.xlabel('Hidden Neurons')
plt.ylabel('Accuracy (%)')

plt.subplot(2, 2, 4)
plt.bar(results['Activation Function'][0], results['Activation Function'][1])
plt.title('Effect of Activation Function on Accuracy')
plt.xlabel('Activation Function')
plt.ylabel('Accuracy (%)')

plt.tight_layout()
plt.show()